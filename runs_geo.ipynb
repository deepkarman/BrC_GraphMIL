{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "structured-continent",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os, sys, glob, pickle\n",
    "import pdb, time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from collections import Counter\n",
    "\n",
    "import torch_geometric\n",
    "import torch_geometric.transforms as T\n",
    "import torch_geometric.nn as geo_nn\n",
    "from torch_geometric.data import Data, Dataset, DataLoader\n",
    "\n",
    "from layers import *\n",
    "import math\n",
    "from itertools import chain\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\"\n",
    "best_auc_fold = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "loaded-approach",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, pickle_files_dir, file_list):\n",
    "        self.pickle_files_dir=pickle_files_dir\n",
    "        self.file_list = file_list\n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        file_name = self.file_list[index]\n",
    "        \n",
    "        # if self.augment:\n",
    "            # TODO\n",
    "            \n",
    "        # print('\\n\\n', file_name, '\\n\\n')\n",
    "        with open(file_name,'rb') as file:\n",
    "            data = pickle.load(file)\n",
    "\n",
    "        vertex = data[0]\n",
    "\n",
    "        edge = data[1]\n",
    "#         edge, _ =  torch_geometric.utils.add_remaining_self_loops(edge)\n",
    "        \n",
    "        l = torch.tensor(data[2])\n",
    "\n",
    "        slide_idx = torch.tensor(data[3], dtype=torch.int)\n",
    "        \n",
    "        data_ =  Data(x=vertex, edge_index=edge, y=l)\n",
    "        data_.slide_idx = slide_idx\n",
    "\n",
    "        return data_\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "defined-going",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------- NETWORK --------------------------\n",
    "\n",
    "\n",
    "class Net_geo(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_geo, self).__init__()\n",
    "\n",
    "        # Conv Layer candidates\n",
    "        # AGNNConv\n",
    "        # ARMAConv\n",
    "        # DynamicEdgeConv\n",
    "        # GATConv\n",
    "        # GCNConv - GINConv better?\n",
    "        # SAGEConv\n",
    "        # SGConv\n",
    "\n",
    "        # Norm Later candidates\n",
    "        # BatchNorm - batch normalization over a batch of node features\n",
    "        # InstanceNorm - instance normalization over each individual example in a batch of node features\n",
    "\n",
    "        # Pooling\n",
    "        # EdgePooling - needs an edge score method\n",
    "        # SAGPooling - needs a GNN for calculating projection scores\n",
    "        # TopKPooling - better than SAGPool\n",
    "        # nearest\n",
    "        # radius\n",
    "\n",
    "        # Global Pool to get graph level output\n",
    "\n",
    "        # TODO\n",
    "        # First input layer - normalise size ... how?\n",
    "        # Node feature conv - reduce number from 438 to more managable\n",
    "        # Edge conv? Edge features? Edge scores from incident nodes? - batchwise too\n",
    "\n",
    "        # self.GCNConv1 = geo_nn.GCNConv(438,512, add_self_loops = False, normalize = True, improved = True, cached = False, bias = True)\n",
    "        # self.bn1 = nn.BatchNorm1d(54)\n",
    "#         self.GINConv1 = geo_nn.GINConv(nn.Sequential(nn.Linear(512, 256), nn.ReLU(), nn.Linear(256, 256)))\n",
    "#         self.GINConv2 = geo_nn.GINConv(nn.Sequential(nn.Linear(256, 128), nn.ReLU(), nn.Linear(128, 128)))\n",
    "#         self.GCNConv1 = geo_nn.GCNConv(512,256, add_self_loops = True, normalize = True, improved = True, cached = False, bias = True)\n",
    "#         self.GCNConv2 = geo_nn.GCNConv(256,128, add_self_loops = True, normalize = True, improved = True, cached = False, bias = True)\n",
    "        # self.GCNConv5 = geo_nn.GCNConv(128,128, add_self_loops = True, normalize = True, improved = True, cached = False, bias = True)\n",
    "#         self.gc1 = geo_nn.GraphConv(512, 256)\n",
    "#         self.gc2 = geo_nn.GraphConv(256, 128)\n",
    "        self.gcn1 = GraphCNNLayer_geo(512,256,1)\n",
    "        self.gcn2 = GraphCNNLayer_geo(256,128,1)\n",
    "\n",
    "        # self.bn1 = nn.BatchNorm1d(128)\n",
    "\n",
    "#         self.SAGPool1 = geo_nn.SAGPooling(128, ratio=0.5) # 586+438=1024\n",
    "#         self.TopKPool1 = geo_nn.TopKPooling(128, ratio=0.5)\n",
    "        self.EdgePool1 = geo_nn.EdgePooling(128) # ratio is always 0.5\n",
    "\n",
    "        # self.GCNConv2 = geo_nn.GCNConv(512, 256, add_self_loops = False, normalize = True, improved = True, cached = False, bias = True)\n",
    "        \n",
    "#         self.GINConv3 = geo_nn.GINConv(nn.Sequential(nn.Linear(128, 128), nn.ReLU(), nn.Linear(128, 128)))\n",
    "#         self.GINConv4 = geo_nn.GINConv(nn.Sequential(nn.Linear(128, 128), nn.ReLU(), nn.Linear(128, 128)))\n",
    "#         self.GCNConv3 = geo_nn.GCNConv(128,128, add_self_loops = True, normalize = True, improved = True, cached = False, bias = True)\n",
    "#         self.GCNConv4 = geo_nn.GCNConv(128,128, add_self_loops = True, normalize = True, improved = True, cached = False, bias = True)\n",
    "        # self.GCNConv6 = geo_nn.GCNConv(256,256, add_self_loops = True, normalize = True, improved = True, cached = False, bias = True)\n",
    "#         self.gc3 = geo_nn.GraphConv(128, 128)\n",
    "#         self.gc4 = geo_nn.GraphConv(128, 128)\n",
    "        self.gcn3 = GraphCNNLayer_geo(128,128,1)\n",
    "        self.gcn4 = GraphCNNLayer_geo(128,128,1)\n",
    "\n",
    "        # self.bn2 = nn.BatchNorm1d(256)\n",
    "\n",
    "        # self.SAGPool2 = geo_nn.SAGPooling(512, ratio=0.5) # 1024+256=1280\n",
    "        # self.TopKPool2 = geo_nn.TopKPooling(256, ratio=0.5)\n",
    "\n",
    "        # self.GCNConv3 = geo_nn.GCNConv(256, 256, add_self_loops = False, normalize = True, improved = True, cached = False, bias = True)\n",
    "        \n",
    "        # self.GINConv5 = geo_nn.GINConv(nn.Sequential(nn.Linear(256, 256), nn.ReLU(), nn.Linear(256, 256)))\n",
    "        # self.GINConv6 = geo_nn.GINConv(nn.Sequential(nn.Linear(256, 256), nn.ReLU(), nn.Linear(256, 256)))\n",
    "        # self.GCNConv5 = geo_nn.GCNConv(256,256, add_self_loops = True, normalize = True, improved = True, cached = False, bias = True)\n",
    "        # self.GCNConv6 = geo_nn.GCNConv(256,256, add_self_loops = True, normalize = True, improved = True, cached = False, bias = True)\n",
    "\n",
    "        # self.bn3 = nn.BatchNorm1d(256)\n",
    "\n",
    "#         self.SAGPool3 = geo_nn.SAGPooling(128, ratio=0.5)\n",
    "#         self.TopKPool3 = geo_nn.TopKPooling(128, ratio=0.5)\n",
    "        self.EdgePool3 = geo_nn.EdgePooling(128) # ratio is always 0.5\n",
    "\n",
    "#         self.GINConv7 = geo_nn.GINConv(nn.Sequential(nn.Linear(128, 128), nn.ReLU(), nn.Linear(128, 128)))\n",
    "#         self.GCNConv7 = geo_nn.GCNConv(128,128, add_self_loops = True, normalize = True, improved = True, cached = False, bias = True)\n",
    "#         self.gc7 = geo_nn.GraphConv(128, 128)\n",
    "        self.gcn7 = GraphCNNLayer_geo(128,128,1)\n",
    "        \n",
    "\n",
    "        self.post_gcn = nn.Sequential(\n",
    "            nn.Linear(128, 128), nn.BatchNorm1d(128), nn.ReLU(), nn.Dropout(), \n",
    "            nn.Linear(128, 128), nn.BatchNorm1d(128), nn.ReLU(), nn.Dropout(),\n",
    "            nn.Linear(128, 64), nn.BatchNorm1d(64), nn.ReLU(), \n",
    "            nn.Linear(64,2)\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, batch, x, edge_index):\n",
    "        # TODO\n",
    "        edge_attr = None\n",
    "        # x = self.bn1(x)\n",
    "#         x = self.GINConv2(self.GINConv1(x, edge_index), edge_index) # x of shape [num_nodes, 512] now\n",
    "        # x = self.GCNConv5(self.GCNConv2(self.GCNConv1(x, edge_index), edge_index), edge_index)\n",
    "#         x = self.GCNConv2(self.GCNConv1(x, edge_index), edge_index)\n",
    "#         x = self.gc2(self.gc1(x, edge_index), edge_index)\n",
    "        x = self.gcn1(x, edge_index, edge_attr, batch)\n",
    "        x = self.gcn2(x, edge_index, edge_attr, batch)\n",
    "        # x = torch.cat((x, self.GINConv2(self.GINConv1(x, edge_index), edge_index)), dim=1) # x of shape [num_nodes, 438+586=1024] now\n",
    "\n",
    "        # x = self.bn1(x)\n",
    "\n",
    "#         x, edge_index, _, batch, _,_ = self.SAGPool1(x, edge_index, batch=batch)\n",
    "#         x, edge_index, _, batch, _,_ = self.TopKPool1(x, edge_index, batch=batch)\n",
    "        x, edge_index, batch, _ = self.EdgePool1(x, edge_index, batch)\n",
    "        \n",
    "\n",
    "#         x = self.GINConv4(self.GINConv3(x, edge_index), edge_index) # x of shape [num_nodes, 512] now \n",
    "        # x = self.GCNConv6(self.GCNConv4(self.GCNConv3(x, edge_index), edge_index), edge_index)\n",
    "#         x = self.GCNConv4(self.GCNConv3(x, edge_index), edge_index)\n",
    "#         x = self.gc4(self.gc3(x, edge_index), edge_index)\n",
    "        x = self.gcn3(x, edge_index, edge_attr, batch)\n",
    "        x = self.gcn4(x, edge_index, edge_attr, batch)\n",
    "        # x = torch.cat((x, self.GINConv4(self.GINConv3(x, edge_index), edge_index)), dim=1) # x of shape [num_nodes, 1024] now \n",
    " \n",
    "        # x = self.bn2(x)\n",
    "\n",
    "        # x, edge_index, edge_attr, batch, _,_ = self.SAGPool2(x, edge_index, edge_attr=edge_attr, batch=batch)\n",
    "        # x, edge_index, _, batch, _,_ = self.TopKPool2(x, edge_index, batch=batch)\n",
    "\n",
    "        # slide_idx = slide_idx[perm]\n",
    "\n",
    "\n",
    "        # x = self.GINConv6(self.GINConv5(x, edge_index), edge_index) # x of shape [num_nodes, 512] now \n",
    "        # x = torch.cat((x, self.GINConv6(self.GINConv5(x, edge_index), edge_index)), dim=1) # x of shape [num_nodes, 1024+512] now \n",
    "        # x = self.GINConv6(self.GINConv5(x, edge_index), edge_index) # x of shape [num_nodes, 512] now\n",
    "        # x = self.GCNConv6(self.GCNConv5(x, edge_index), edge_index)\n",
    "\n",
    "        \n",
    "        # x = self.bn3(x)\n",
    "        \n",
    "#         x, edge_index, _, batch, _,_ = self.SAGPool3(x, edge_index, batch=batch)\n",
    "#         x, edge_index, _, batch, _,_ = self.TopKPool3(x, edge_index, batch=batch)\n",
    "        x, edge_index, batch, _ = self.EdgePool3(x, edge_index, batch)\n",
    "\n",
    "#         x = self.GINConv7(x, edge_index) # x of shape [num_nodes, 512]\n",
    "#         x = self.GCNConv7(x, edge_index)\n",
    "#         x = self.gc7(x, edge_index)\n",
    "        x = self.gcn7(x, edge_index, edge_attr, batch)\n",
    "        # x = torch.cat((x, self.GINConv7(x, edge_index)), dim=1) # x of shape [num_nodes, 2048]\n",
    "\n",
    "        x = geo_nn.global_mean_pool(x, batch) # x of shape [1, 128] now - for each graph in batch\n",
    "        # x = geo_nn.global_max_pool(x, batch) # x of shape [1, 128] now - for each graph in batch\n",
    "\n",
    "        x = self.post_gcn(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fourth-conclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_acc_and_auc(model, device, data_loader, loss_ce, plot = False):\n",
    "\n",
    "    y_score = []\n",
    "    y_true  = []\n",
    "    y_pred = []\n",
    "    m = nn.Softmax(dim = 1)\n",
    "    correct_preds = 0\n",
    "\n",
    "    for batch_idx, data_ in enumerate(data_loader):\n",
    "        with torch.no_grad():\n",
    "            batch = data_['batch'].to(device)\n",
    "            edge_index = data_['edge_index'].to(device)\n",
    "            slide_idx = data_['slide_idx'].to(device)\n",
    "            x_data = data_['x'].to(device)\n",
    "            y_target = data_['y'].to(device)\n",
    "\n",
    "            model.eval()\n",
    "\n",
    "            output = model(batch, x_data, edge_index) \n",
    "            \n",
    "            preds = output.max(1, keepdim=True)[1]\n",
    "            correct_preds += preds.eq(y_target.view_as(preds)).sum().item()\n",
    "            \n",
    "            y_score.append(np.array(m(output).cpu()[:,1]).tolist())\n",
    "            y_true.append(np.array(y_target.cpu()).tolist())\n",
    "            y_pred.append(np.array(m(output).max(1,keepdim=True)[1].cpu()).tolist())\n",
    "\n",
    "            del batch\n",
    "            del slide_idx\n",
    "            del edge_index\n",
    "            del x_data       \n",
    "            del y_target \n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    try:\n",
    "        y_true = list(chain(*y_true))\n",
    "        y_score = list(chain(*y_score))\n",
    "        y_pred = list(chain(*y_pred))\n",
    "        auc = roc_auc_score(y_true,y_score) # TODO - check this\n",
    "        # pdb.set_trace()\n",
    "    except ValueError as e:\n",
    "        print(\"auc has error\")\n",
    "        print(e)\n",
    "        pdb.set_trace()\n",
    "        auc = 0\n",
    "\n",
    "    if plot:\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "        roc_auc = sklearn.metrics.auc(fpr, tpr)\n",
    "        plt.figure()\n",
    "        lw = 2\n",
    "        plt.plot(fpr, tpr, color='darkorange',\n",
    "                 lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver operating characteristic example')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        print('showing plot')\n",
    "        plt.savefig('auc_roc_curve_'+str(data_loader)+'.png')\n",
    "    acc = 100.*correct_preds/len(data_loader.dataset)\n",
    "    return acc, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "controlled-chamber",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, device, loss_ce, train_loader, optimizer, epoch, test_loader):\n",
    "    global best_auc_fold\n",
    "\n",
    "    running_loss = 0.\n",
    "    correct_preds = 0\n",
    "    elements = 0\n",
    "\n",
    "    for batch_idx, data_ in enumerate(train_loader):\n",
    "\n",
    "        batch = data_['batch'].to(device)\n",
    "        slide_idx = data_['slide_idx'].to(device)\n",
    "        edge_index = data_['edge_index'].to(device)\n",
    "        x_data = data_['x'].to(device)\n",
    "        y_target = data_['y'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # for idx_iter in range(args['num_iter']):\n",
    "        model.train()\n",
    "\n",
    "        output = model(batch, x_data, edge_index) \n",
    "        \n",
    "        loss = loss_ce(output, y_target) \n",
    "        preds = output.max(1, keepdim=True)[1]\n",
    "        elements += len(y_target)\n",
    "        train_batch_corr = preds.eq(y_target.view_as(preds)).sum().item()\n",
    "        train_batch_accuracy = (100*train_batch_corr)/len(y_target)\n",
    "        correct_preds += train_batch_corr\n",
    "        # pdb.set_trace()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "        curr_loss = loss.item()\n",
    "        running_loss += loss.item()*data_.num_graphs\n",
    "        idx = batch_idx + (epoch-1)*math.ceil(len(train_loader))\n",
    "#         writer.add_scalar('Train Loss', curr_loss, idx)  \n",
    "#         writer.add_scalar('Train Batch Accuracy', train_batch_accuracy, idx)\n",
    "        # if epoch == 10:\n",
    "        #     pdb.set_trace()\n",
    "\n",
    "        del batch\n",
    "        del slide_idx\n",
    "        del edge_index\n",
    "        del x_data       \n",
    "        del y_target \n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if batch_idx % args['log_interval'] == 0 :\n",
    "            acc_test, auc_test = calc_acc_and_auc(model, device, test_loader, loss_ce)\n",
    "\n",
    "#             writer.add_scalar('Test Acc', acc_test, idx)\n",
    "#             writer.add_scalar('AUC', auc_test, idx)\n",
    "\n",
    "            print('Train Epoch: {} [({:.0f}%)]\\tLoss: {:.6f} \\tAcc Train: {:.2f}% \\tAcc test {:.2f}\\tTest AUC: {:.3f}'.format(\n",
    "                epoch, 100. * (batch_idx+1) / len(train_loader), curr_loss, 100.*correct_preds/elements,acc_test, auc_test))\n",
    "\n",
    "            if auc_test>best_auc_fold and auc_test > 0.60:\n",
    "                best_auc_fold = auc_test\n",
    "#                 torch.save({\n",
    "#                     'epoch': epoch + 1,\n",
    "#                     'model_state_dict': model.state_dict(),\n",
    "#                     'optimizer_state_dict': optimizer.state_dict(),\n",
    "#                     'auc' : auc_test,\n",
    "#                     }, args['resume'][fold]+'_testauc_'+str(auc_test)+'_new.pth.tar')\n",
    "                print('Saving best AUC: '+ str(auc_test))\n",
    "\n",
    "    running_loss /= len(train_loader.dataset)\n",
    "    print(\"Loss for epoch {} is: {:.6f} \\tAccuracy for Epoch is {:.2f}\".format(epoch, running_loss, 100.*correct_preds/elements))\n",
    "#     writer.add_scalar('Train loss epoch', running_loss, epoch)\n",
    "\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "desirable-stream",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_geo(\n",
      "  (gcn1): GraphCNNLayer_geo(\n",
      "    (lin_A): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (lin_I): Linear(in_features=512, out_features=256, bias=False)\n",
      "    (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU()\n",
      "  )\n",
      "  (gcn2): GraphCNNLayer_geo(\n",
      "    (lin_A): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (lin_I): Linear(in_features=256, out_features=128, bias=False)\n",
      "    (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU()\n",
      "  )\n",
      "  (EdgePool1): EdgePooling(128)\n",
      "  (gcn3): GraphCNNLayer_geo(\n",
      "    (lin_A): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (lin_I): Linear(in_features=128, out_features=128, bias=False)\n",
      "    (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU()\n",
      "  )\n",
      "  (gcn4): GraphCNNLayer_geo(\n",
      "    (lin_A): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (lin_I): Linear(in_features=128, out_features=128, bias=False)\n",
      "    (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU()\n",
      "  )\n",
      "  (EdgePool3): EdgePooling(128)\n",
      "  (gcn7): GraphCNNLayer_geo(\n",
      "    (lin_A): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (lin_I): Linear(in_features=128, out_features=128, bias=False)\n",
      "    (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU()\n",
      "  )\n",
      "  (post_gcn): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.5, inplace=False)\n",
      "    (8): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (9): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): Linear(in_features=64, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# ------------------- Create args ----------------------\n",
    "global best_auc_fold\n",
    "args = {}\n",
    "args['batch_size'] = 200\n",
    "args['test_batch_size'] = 200\n",
    "args['no_cuda']=False\n",
    "args['seed']=7\n",
    "args['lr']=0.01\n",
    "# args['momentum']=0.5\n",
    "args['log_interval']=5\n",
    "args['epochs']=15 # epochs per fold\n",
    "args['num_iter']=1\n",
    "args['start_epoch']=1\n",
    "use_cuda = not args['no_cuda'] and torch.cuda.is_available()\n",
    "torch.manual_seed(args['seed'])\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "kwargs = {'num_workers': 4, 'pin_memory': True} if use_cuda else {}\n",
    "# args['resume']=['checkpoints_TCGA/discard_TCGA_split2_initGCN5_noVGG_topK_.pth.tar' for fold in range(no_of_folds)]\n",
    "# args['resume']=['checkpoints_TCGA/TCGA_run_54params_topK_initBN_folds_0.pth.tar_auc_0.7935537982266956_new.pth.tar']\n",
    "acccuracies = []\n",
    "max_accuracies = []\n",
    "\n",
    "best_auc_fold = 0\n",
    "\n",
    "\n",
    "# ----------------------------Create dataset ----------------------------------------------\n",
    "\n",
    "train_file_list = pickle.load(open('test_train_split/train_file_seed_2.pickle','rb')) \n",
    "test_file_list = pickle.load(open('test_train_split/test_file_seed_2.pickle','rb'))\n",
    "\n",
    "dataset_train = GraphDataset('data_graphs/', train_file_list)\n",
    "dataset_test = GraphDataset('data_graphs/', test_file_list)\n",
    "\n",
    "train_loader = DataLoader(dataset = dataset_train, batch_size = args['batch_size'], shuffle = True, **kwargs) # no collate req; drop_last?; kwarg? \n",
    "test_loader = DataLoader(dataset = dataset_test, batch_size = args['test_batch_size'], shuffle = True, **kwargs)\n",
    "\n",
    "\n",
    "# ---------------------Create model and other primitives --------------------------------------\n",
    "\n",
    "model = Net_geo().float() # model at float32 precision\n",
    "# batchnorm needs accumulators to be big else they'll overflow\n",
    "# so set those to float32\n",
    "for layer in model.modules():\n",
    "    if isinstance(layer, nn.BatchNorm2d):\n",
    "        layer.float32()\n",
    "model = model.to(device)\n",
    "print(model)\n",
    "# pdb.set_trace()\n",
    "# model = nn.DataParallel(model, device_ids=[1, 2]).to(device)\n",
    "\n",
    "# optimizer = optim.SGD(model.parameters(), lr=args['lr'], momentum=args['momentum'],weight_decay=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=args['lr'])\n",
    "\n",
    "#optimizer = optim.Adam(model.parameters(), lr=args['lr'],weight_decay=0)\n",
    "\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 1, gamma = 0.707 , last_epoch=-1)\n",
    "\n",
    "# keep loss as float32 else it will overflow\n",
    "loss_ce = nn.CrossEntropyLoss(weight = torch.tensor([0.972, 1.028],  dtype = torch.float32, device = device), reduction = 'sum') # Train on TCGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "wired-campaign",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File \"/home/karman/anaconda3/envs/geo37/lib/python3.7/site-packages/torch_scatter/scatter.py\", line 22, in scatter_add\n            size[dim] = int(index.max()) + 1\n        out = torch.zeros(size, dtype=src.dtype, device=src.device)\n        return out.scatter_add_(dim, index, src)\n               ~~~~~~~~~~~~~~~~ <--- HERE\n    else:\n        return out.scatter_add_(dim, index, src)\nRuntimeError: CUDA error: device-side assert triggered\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-422b87f9e6d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'start_epoch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_ce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-23be15126adb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, model, device, loss_ce, train_loader, optimizer, epoch, test_loader)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_ce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/geo37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-d9c7274edd91>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batch, x, edge_index)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;31m#         x = self.GCNConv4(self.GCNConv3(x, edge_index), edge_index)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;31m#         x = self.gc4(self.gc3(x, edge_index), edge_index)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_attr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcn4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_attr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# x = torch.cat((x, self.GINConv4(self.GINConv3(x, edge_index), edge_index)), dim=1) # x of shape [num_nodes, 1024] now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/geo37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DDP/BrC_GraphMIL/layers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, edge_attr, batch)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# x: (batch*N) x F\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# V: batch x N x F\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch_geo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dense_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# A: batch x NL x N\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/geo37/lib/python3.7/site-packages/torch_geometric/utils/to_dense_batch.py\u001b[0m in \u001b[0;36mto_dense_batch\u001b[0;34m(x, batch, fill_value, max_num_nodes)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0,\n\u001b[0;32m---> 37\u001b[0;31m                             dim_size=batch_size)\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mcum_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_zeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File \"/home/karman/anaconda3/envs/geo37/lib/python3.7/site-packages/torch_scatter/scatter.py\", line 22, in scatter_add\n            size[dim] = int(index.max()) + 1\n        out = torch.zeros(size, dtype=src.dtype, device=src.device)\n        return out.scatter_add_(dim, index, src)\n               ~~~~~~~~~~~~~~~~ <--- HERE\n    else:\n        return out.scatter_add_(dim, index, src)\nRuntimeError: CUDA error: device-side assert triggered\n"
     ]
    }
   ],
   "source": [
    "# if args['resume'] is not None:\n",
    "#     if os.path.isfile(args['resume'][fold]):\n",
    "#         print(\"=> loading checkpoint '{}'\".format(args['resume'][fold]))\n",
    "#         checkpoint = torch.load(args['resume'][fold])\n",
    "#         args['start_epoch'] = checkpoint['epoch']\n",
    "#         # best_prec1 = checkpoint['best_prec1']\n",
    "#         # print(\"best_prec is \", best_prec1)\n",
    "#         model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#         optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "#         # scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "#         acc, auc = calc_acc_and_auc(model, device, test_loader, loss_ce, plot=True)\n",
    "#         print('acc test is {:.6f}, auc on test is {:.2f}'.format(acc, auc))\n",
    "#         acc, auc = calc_acc_and_auc(model, device, test_loader_JH, loss_ce, plot=True)\n",
    "#         print('acc JH is {:.6f}, auc on JH is {:.2f}'.format(acc, auc))\n",
    "#         sys.exit(0)\n",
    "#     else:\n",
    "#         print(\"=> no checkpoint found at '{}'\".format(args['resume'][fold]))\n",
    "#         # sys.exit(0)\n",
    "\n",
    "for epoch in range(args['start_epoch'], args['epochs'] + 1):\n",
    "    \n",
    "    train(args, model, device, loss_ce, train_loader, optimizer, epoch, test_loader)\n",
    "    scheduler.step()\n",
    "\n",
    "    # torch.save({\n",
    "    #     'epoch': epoch + 1,\n",
    "    #     'model_state_dict': model.state_dict(),\n",
    "    #     'optimizer_state_dict': optimizer.state_dict(),\n",
    "    #     'scheduler': scheduler.state_dict(),\n",
    "    #     }, args['resume'][fold])\n",
    "\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Best auc: \", best_auc_fold)\n",
    "\n",
    "for k, v in args.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "existing-diagram",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_auc_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "angry-dictionary",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:geo37]",
   "language": "python",
   "name": "conda-env-geo37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
